{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72123fa",
   "metadata": {},
   "source": [
    "### just an experimentation of how ann with simple attetnion weights will work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0990878a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: X=(2408, 26), y=(2408,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and prepare data (same as your code)\n",
    "LONG_METHOD_dataset = pd.read_csv(\"../../../dataset-source/embedding-dataset/software-metrics/LongMethod_code_metrics_values.csv\")\n",
    "X_LONG_METHOD_dataset, y_LONG_METHOD_dataset = LONG_METHOD_dataset.drop(['sample_id','label'], axis=1).values, LONG_METHOD_dataset['label'].values\n",
    "print(f\"Dataset shape: X={X_LONG_METHOD_dataset.shape}, y={y_LONG_METHOD_dataset.shape}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_LONG_METHOD_dataset, y_LONG_METHOD_dataset, \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train).view(-1, 1)  # Shape: (n, 1)\n",
    "y_test = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "input_features = X_LONG_METHOD_dataset.shape[1]  # 26\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12837c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeatureAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Learns attention weights for each input feature.\n",
    "    Weights sum to 1.0 and indicate feature importance.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features, attention_dim=32):\n",
    "        super().__init__()\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(input_features, attention_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_dim, input_features),\n",
    "            nn.Softmax(dim=1)  # Ensures weights sum to 1\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention_weights = self.attention_layer(x)  # Shape: (batch, 26)\n",
    "        weighted_features = x * attention_weights     # Element-wise multiplication\n",
    "        return weighted_features, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1135dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Model_With_Attention(nn.Module):\n",
    "    def __init__(self, input_features=26, hidden1=64, hidden2=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Attention mechanism for explainability\n",
    "        self.attention = SimpleFeatureAttention(input_features, attention_dim=32)\n",
    "        \n",
    "        # Neural network layers\n",
    "        self.f_connected1 = nn.Linear(input_features, hidden1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.f_connected2 = nn.Linear(hidden1, hidden2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(hidden2, 1)  # CHANGED: 1 output for binary classification\n",
    "        \n",
    "        # Store attention weights for analysis\n",
    "        self.last_attention_weights = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply attention to learn feature importance\n",
    "        attended_x, attention_weights = self.attention(x)\n",
    "        self.last_attention_weights = attention_weights  # Save for explainability\n",
    "        \n",
    "        # Forward pass through network\n",
    "        x = F.relu(self.f_connected1(attended_x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.f_connected2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.sigmoid(self.out(x)) \n",
    "        return x\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Extract average attention weights across batch\"\"\"\n",
    "        if self.last_attention_weights is not None:\n",
    "            return self.last_attention_weights.mean(dim=0).detach()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d3474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Epoch 50/500 - Loss: 0.399996\n",
      "Epoch 100/500 - Loss: 0.172375\n",
      "Epoch 150/500 - Loss: 0.107078\n",
      "Epoch 200/500 - Loss: 0.086150\n",
      "Epoch 250/500 - Loss: 0.072311\n",
      "Epoch 300/500 - Loss: 0.057973\n",
      "Epoch 350/500 - Loss: 0.050287\n",
      "Epoch 400/500 - Loss: 0.040403\n",
      "Epoch 450/500 - Loss: 0.035104\n",
      "Epoch 500/500 - Loss: 0.027991\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(20)\n",
    "model = ANN_Model_With_Attention(input_features=input_features, hidden1=64, hidden2=32)\n",
    "\n",
    "\n",
    "loss_fn = nn.BCELoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 500\n",
    "final_losses = []\n",
    "\n",
    "print(\"Training started...\")\n",
    "model.train()\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    final_losses.append(loss.item())\n",
    "    \n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"Epoch {i+1}/{epochs} - Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    # Backward propagation\n",
    "    optimizer.zero_grad() # clears old gradient\n",
    "    loss.backward() # calculates the gradients of the loss wrt to each model parameter\n",
    "    optimizer.step() # it updates the model params using calcualted gradients\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46019056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.655094\n",
      "Test Accuracy: 90.66%\n",
      "\n",
      "======================================================================\n",
      "FEATURE IMPORTANCE (from Attention Weights)\n",
      "======================================================================\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "1. uniqueWordsQty: 0.418209\n",
      "2. loc: 0.265773\n",
      "3. variablesQty: 0.125396\n",
      "4. comparisonsQty: 0.012094\n",
      "5. modifiers: 0.011792\n",
      "6. stringLiteralsQty: 0.011322\n",
      "7. maxNestedBlocksQty: 0.011115\n",
      "8. parametersQty: 0.010793\n",
      "9. assignmentsQty: 0.010504\n",
      "10. wmc: 0.010406\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "model.eval()\n",
    "# model.eval() sets the model to evaluation mode, turning off dropout and using running averages \n",
    "# in batch normalization for stable, consistent results during inference. Without it, dropout stays\n",
    "# active and batch norm uses batch stats, causing random or inconsistent predictions .\n",
    "\n",
    "with torch.no_grad(): # making sure that not to track thje gradients for any operations as it saves memory\n",
    "    y_test_pred = model(X_test)\n",
    "    test_loss = loss_fn(y_test_pred, y_test)\n",
    "    y_test_pred_class = (y_test_pred >= 0.5).float()\n",
    "    accuracy = (y_test_pred_class == y_test).float().mean()\n",
    "    \n",
    "    print(f\"\\nTest Loss: {test_loss.item():.6f}\")\n",
    "    print(f\"Test Accuracy: {accuracy.item()*100:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea1883",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE (from Attention Weights)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(X_test)\n",
    "    avg_attention = model.get_feature_importance()\n",
    "    \n",
    "    feature_importance = avg_attention.numpy()\n",
    "    sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "    \n",
    "    feature_names = LONG_METHOD_dataset.drop(['sample_id', 'label'], axis=1).columns\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    for rank, idx in enumerate(sorted_indices[:10], 1):\n",
    "        print(f\"{rank}. {feature_names[idx]}: {feature_importance[idx]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc0fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ATTENTION WEIGHTS FOR INDIVIDUAL SAMPLES\n",
      "======================================================================\n",
      "\n",
      "Sample 1:\n",
      "  True Label: 0\n",
      "  Predicted: 0.0030 (class: 0)\n",
      "  Top 5 Features Influencing This Prediction:\n",
      "    1. uniqueWordsQty: 0.4764\n",
      "    2. loc: 0.3107\n",
      "    3. variablesQty: 0.0753\n",
      "    4. modifiers: 0.0098\n",
      "    5. stringLiteralsQty: 0.0098\n",
      "\n",
      "Sample 2:\n",
      "  True Label: 0\n",
      "  Predicted: 0.0009 (class: 0)\n",
      "  Top 5 Features Influencing This Prediction:\n",
      "    1. uniqueWordsQty: 0.4014\n",
      "    2. loc: 0.2887\n",
      "    3. variablesQty: 0.0501\n",
      "    4. comparisonsQty: 0.0189\n",
      "    5. modifiers: 0.0189\n",
      "\n",
      "Sample 3:\n",
      "  True Label: 0\n",
      "  Predicted: 0.0003 (class: 0)\n",
      "  Top 5 Features Influencing This Prediction:\n",
      "    1. uniqueWordsQty: 0.5994\n",
      "    2. loc: 0.1164\n",
      "    3. variablesQty: 0.0283\n",
      "    4. methodsInvokedIndirectLocalQty: 0.0171\n",
      "    5. parametersQty: 0.0155\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ATTENTION WEIGHTS FOR INDIVIDUAL SAMPLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze first 3 test samples\n",
    "with torch.no_grad():\n",
    "    sample_predictions = model(X_test[:3])\n",
    "    sample_attention = model.last_attention_weights[:3]\n",
    "    \n",
    "for i in range(3):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  True Label: {y_test[i].item():.0f}\")\n",
    "    print(f\"  Predicted: {sample_predictions[i].item():.4f} (class: {(sample_predictions[i] >= 0.5).float().item():.0f})\")\n",
    "    \n",
    "    top5_idx = torch.argsort(sample_attention[i], descending=True)[:5]\n",
    "    print(f\"  Top 5 Features Influencing This Prediction:\")\n",
    "    for rank, idx in enumerate(top5_idx, 1):\n",
    "        print(f\"    {rank}. {feature_names[idx.item()]}: {sample_attention[i][idx].item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
